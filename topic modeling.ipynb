{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0a317f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sherryliu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "nltk.download('punkt') \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import asarray\n",
    "from numpy import array\n",
    "from numpy import zeros\n",
    "from typing import List\n",
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9f6a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9dc5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tinder_google_play_reviews.csv', usecols=['at', 'content', 'reviewCreatedVersion'])\n",
    "df.dropna(subset=['content'],inplace=True)\n",
    "df.dropna(subset=['reviewCreatedVersion'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f8366a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"It was positively good and I met with lot of ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Major user policy needed change As a men tryi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fake</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Better</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not so good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>\"O app não é ruim, mas não entendi pq excluira...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Good</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Dont bother. Itll say \"\"you got a match\"\" and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>It doesn't unmanned you after awhile</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>\"Matched up with several women that I was inte...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews scores\n",
       "0    \"It was positively good and I met with lot of ...      5\n",
       "1    \"Major user policy needed change As a men tryi...      1\n",
       "2                                                 Fake      1\n",
       "3                                               Better      5\n",
       "4                                          Not so good      3\n",
       "..                                                 ...    ...\n",
       "994  \"O app não é ruim, mas não entendi pq excluira...      2\n",
       "995                                               Good      4\n",
       "996  \"Dont bother. Itll say \"\"you got a match\"\" and...      1\n",
       "997               It doesn't unmanned you after awhile      1\n",
       "998  \"Matched up with several women that I was inte...      1\n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### if we would like to use a bigger dataset, use this code instead\n",
    "#def get_rows(i):\n",
    "#    '''input the number of rows you need'''\n",
    "#    file = open('tinder_google_play_reviews.csv', \"r\")\n",
    "#    review = []\n",
    "#    score = []\n",
    "#    for i in range(i):\n",
    "#        line = file.readline()\n",
    "#        match = regex.search(r'(?<=\\,.+\\,.+\\,)(.+)(?=\\,[12345]\\,)', line)\n",
    "#        if match:\n",
    "#            review.append(match.group())\n",
    "#            match_s = regex.search(r'(?<=\\,.+\\,.+\\,.+\\,)([12345])(?=\\,)', line)\n",
    "#            if match_s:\n",
    "#                score.append(match_s.group())\n",
    "#    d = {'content': review, 'score': score}\n",
    "#    df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e3224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('at', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56c29eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110770, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['at'] = pd.to_datetime(df['at'], utc=True)\n",
    "df['year'] = df['at'].apply(lambda r:r.year)\n",
    "df = df[df['year'] > 2019]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "80f51d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content = df.content.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8a6af125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4f/2_g2k8l95078qdts2g05fkf00000gn/T/ipykernel_1042/4057811400.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.content = df.content.str.replace(r\"\\b\\w+n't\\b\",'not', flags=re.IGNORECASE)\n",
      "/var/folders/4f/2_g2k8l95078qdts2g05fkf00000gn/T/ipykernel_1042/4057811400.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.content = df.content.str.replace(r'\\b(?:awesome|great|excellent|wonderful|well)\\b', 'good', flags=re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "#regex cleaning\n",
    "df.content = df.content.str.replace(r\"\\b\\w+n't\\b\",'not', flags=re.IGNORECASE)\n",
    "df.content = df.content.str.replace(r'\\b(?:awesome|great|excellent|wonderful|well)\\b', 'good', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8af5578a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 110770/110770 [01:29<00:00, 1233.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = list(stopwords.words('english'))\n",
    "#we notice from words like app are commonly occuring but dont contribute to the meaning\n",
    "stopwords = stopwords + ['app', 'apps', 'people','literally','really','very','definitely'] # delete tinder and account since we need them for regex\n",
    "stopwords.remove('not')\n",
    "lines = list(df['content'])\n",
    "cleaned_reviews = []\n",
    "\n",
    "## Reference: https://gist.github.com/gaurav5430/9fce93759eb2f6b1697883c3782f30de#file-nltk-lemmatize-sentences-py\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "def lem(sentence):\n",
    "    '''Intake a list of review, lemmatize them and return a new list'''\n",
    "    result_lem = []\n",
    "    for s in tqdm(sentence):\n",
    "        s_lem = lemmatize_sentence(s)\n",
    "        result_lem.append(s_lem)\n",
    "    return result_lem\n",
    "\n",
    "cleaned_reviews = lem(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "515216db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = cleaned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "03930003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    it be positively good and i meet with lot of n...\n",
       "1    major user policy need change as a men try the...\n",
       "3                                                 well\n",
       "4                                          not so good\n",
       "5    ton of fake profile and everythings behind a p...\n",
       "Name: cleaned_text, dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textacy.preprocessing.replace import urls, hashtags, numbers, emails, emojis, currency_symbols\n",
    "df[\"cleaned_text\"] = df.cleaned_text.\\\n",
    "  apply(urls).\\\n",
    "  apply(hashtags).\\\n",
    "  apply(numbers).\\\n",
    "  apply(currency_symbols).\\\n",
    "  apply(emojis).\\\n",
    "  apply(emails)\n",
    "df.cleaned_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a1839aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(line):\n",
    "    '''regex cleaning for common product issues'''\n",
    "    line = re.sub(r'\\b(ban(?:ned)?|suspend(?:ed)?|b?lock(?:ed)?)\\b', '_BAN_', line)\n",
    "    line = re.sub(r'\\b(bug(?:gy)?|crash|server|algorithm)\\b', '_BUG_', line)\n",
    "    line = re.sub(r'\\b(regist(?:ration|er|ered))\\b', '_REGISTER_', line)\n",
    "    line = re.sub(r'\\b(log(?:ging|ged)?(?:-| )?in)\\b', '_LOGIN_', line)\n",
    "    line = re.sub(r'\\b(faked?|bot|not real)\\b','_FAKE_', line)\n",
    "    line = re.sub(r'\\b(match(?:ing)?)\\b','_MATCHING_', line)\n",
    "    line = re.sub(r'\\b(location|distance)\\b','_LOCATION_', line)\n",
    "    line = re.sub(r'\\b(subscript(?:ion)?|charg(?:ed|ing|es)?|pay(?: )?walls?|expensive|premium|in-app-purchase|tinder gold|membership|payment)\\b','_CHARGES_', line)\n",
    "    line = re.sub(r'\\b(notification|spam(?:med)?|promotion)\\b','_SPAM_', line)\n",
    "    line = re.sub(r'\\b(scam(?:mer|med)?)\\b','_SCAM_', line)\n",
    "    line = re.sub(r'\\bsuper likes?\\b','_SUPER_LIKE_', line)\n",
    "    line = re.sub(r'\\blikes?\\b','_LIKE_', line)\n",
    "    line = re.sub(r'\\brewinds?\\b','_REWIND_', line)\n",
    "    line = re.sub(r'\\bpassport\\b','_PASSPORT_', line)\n",
    "    line = re.sub(r'\\bmessages?\\b','_MESSAGE_', line)\n",
    "    line = re.sub(r'\\bswipe(s| left| right)?\\b','_SWIPE_', line)\n",
    "    line = re.sub(r'\\btop picks?\\b','_TOP_PICK_', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "800eb394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it be positively good and i meet with lot of nice , interesting , fun , loveable people , but there be a few pervert who on profile be not what they claim to be .',\n",
       " 'major user policy need change as a men try the apps , let me tell you it have full of broken soul , become an instagram follower mining , lot of wahmen will try to _SCAM_ you ... and as good not to mention the professional _SCAM_ that exist in most dating apps . if tinder still want to be relevant in these climate , major policy / user interaction need change . it have become a business platform for majority of the oposite sex . when apps not longer serve it purpose , it will die .',\n",
       " 'well',\n",
       " 'not so good',\n",
       " 'ton of _FAKE_ profile and everythings behind a _CHARGES_ . why would anyone be foolish enough to pay for a date app with so many _FAKE_ profile .']"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_all = df['cleaned_text'].tolist()\n",
    "regex_1 = [regex(r) for r in review_all]\n",
    "regex_1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5178f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vec(review, i):\n",
    "    '''Intake a list of review and return tf-idf report'''\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(i,i),\n",
    "                             token_pattern=r'\\b[a-zA-Z\\_]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopwords, max_features=1000, binary=True)\n",
    "    X = vectorizer.fit_transform(review)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "    tf_idf_sum = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf_sum, columns=[\"score\"])\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    nmf = NMF(n_components=4)\n",
    "    W = nmf.fit_transform(X)\n",
    "    H = nmf.components_\n",
    "    tf_idf_topic = pd.DataFrame(X.toarray(), columns=terms)\n",
    "    def get_top_tf_idf_tokens_for_topic(H: np.array, feature_names: List[str], num_top_tokens: int = 5):\n",
    "        for topic, vector in enumerate(H):\n",
    "            print(f\"TOPIC {topic}\\n\")\n",
    "            total = vector.sum()\n",
    "            top_scores = vector.argsort()[::-1][:num_top_tokens]\n",
    "            token_names = list(map(lambda idx: feature_names[idx], top_scores))\n",
    "            strengths = list(map(lambda idx: vector[idx] / total, top_scores))\n",
    "            for strength, token_name in zip(strengths, token_names):\n",
    "                print(f\"\\b{token_name} ({round(strength * 100, 1)}%)\\n\")\n",
    "            print(f\"=\" * 50)\n",
    "    get_top_tf_idf_tokens_for_topic(H, tf_idf_topic.columns.tolist(), 5)\n",
    "    return W, H, tf_idf.transpose(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f74ea407",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['regex_1'] = regex_1\n",
    "review_good = df[df['score'] >= 4]['regex_1']\n",
    "review_bad = df[df['score'] < 4]['regex_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0e226979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryliu/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\n",
      "\bget _ban_ reason (46.1%)\n",
      "\n",
      "\baccount get _ban_ (12.0%)\n",
      "\n",
      "\b_ban_ reason not (3.8%)\n",
      "\n",
      "\bkeep get _ban_ (1.3%)\n",
      "\n",
      "\breason not even (0.9%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\bnot waste time (34.7%)\n",
      "\n",
      "\bwaste time money (27.4%)\n",
      "\n",
      "\bdont waste time (1.3%)\n",
      "\n",
      "\bplease not waste (0.8%)\n",
      "\n",
      "\bcomplete waste time (0.7%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\bmany _fake_ profile (67.2%)\n",
      "\n",
      "\bway many _fake_ (3.3%)\n",
      "\n",
      "\b_fake_ profile _scam_ (1.9%)\n",
      "\n",
      "\b_fake_ profile _fake_ (1.9%)\n",
      "\n",
      "\b_fake_ profile not (1.6%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 3\n",
      "\n",
      "\bnot get _matching_ (50.9%)\n",
      "\n",
      "\bget _matching_ not (2.5%)\n",
      "\n",
      "\bstill not get (1.7%)\n",
      "\n",
      "\bget _matching_ even (1.6%)\n",
      "\n",
      "\bget _matching_ unless (1.3%)\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>get _ban_ reason</th>\n",
       "      <td>443.109855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not waste time</th>\n",
       "      <td>318.406177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account get _ban_</th>\n",
       "      <td>297.933534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many _fake_ profile</th>\n",
       "      <td>296.490275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not get _matching_</th>\n",
       "      <td>291.554996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste time money</th>\n",
       "      <td>287.460441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_number_ mile away</th>\n",
       "      <td>275.007478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account _ban_ reason</th>\n",
       "      <td>213.860982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay see _like_</th>\n",
       "      <td>207.275449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got _ban_ reason</th>\n",
       "      <td>204.497896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ account reason</th>\n",
       "      <td>193.111739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ reason not</th>\n",
       "      <td>172.140751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give _number_ star</th>\n",
       "      <td>171.223803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ without reason</th>\n",
       "      <td>159.181822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>many _fake_ account</th>\n",
       "      <td>149.960047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lot _fake_ profile</th>\n",
       "      <td>147.595529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not even get</th>\n",
       "      <td>146.939275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not waste money</th>\n",
       "      <td>138.237513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account _ban_ not</th>\n",
       "      <td>125.368022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get _ban_ not</th>\n",
       "      <td>125.272611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "get _ban_ reason      443.109855\n",
       "not waste time        318.406177\n",
       "account get _ban_     297.933534\n",
       "many _fake_ profile   296.490275\n",
       "not get _matching_    291.554996\n",
       "waste time money      287.460441\n",
       "_number_ mile away    275.007478\n",
       "account _ban_ reason  213.860982\n",
       "pay see _like_        207.275449\n",
       "got _ban_ reason      204.497896\n",
       "_ban_ account reason  193.111739\n",
       "_ban_ reason not      172.140751\n",
       "give _number_ star    171.223803\n",
       "_ban_ without reason  159.181822\n",
       "many _fake_ account   149.960047\n",
       "lot _fake_ profile    147.595529\n",
       "not even get          146.939275\n",
       "not waste money       138.237513\n",
       "account _ban_ not     125.368022\n",
       "get _ban_ not         125.272611"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, H, tf_idf_bad, score_bad = get_tfidf_vec(review_bad, 3)\n",
    "score_bad[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "9166d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_documents_for_each_topic(W: np.array, documents: List[str], num_docs: int = 10):\n",
    "    sorted_docs = W.argsort(axis=0)[::-1]\n",
    "    top_docs = sorted_docs[:num_docs].T\n",
    "    per_document_totals = W.sum(axis=1)\n",
    "    for topic, top_documents_for_topic in enumerate(top_docs):\n",
    "        print(f\"Topic {topic}\")\n",
    "        for doc in top_documents_for_topic:\n",
    "            score = W[doc][topic]\n",
    "            percent_about_topic = round(score / per_document_totals[doc] * 100, 1)\n",
    "            print(f\"{percent_about_topic}%\")\n",
    "            print(documents[doc].replace('\\n',''))\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "68a90fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0%\n",
      "i get _BAN_ for no reason lmao\n",
      "100.0%\n",
      "i get _BAN_ for no reason , please tell me how i can fix this issue _EMOJI__EMOJI_\n",
      "100.0%\n",
      "i get _BAN_ for no reason . this app be stupid\n",
      "100.0%\n",
      "bore get _BAN_ for no reason too\n",
      "100.0%\n",
      "i get _BAN_ for no reason . i be nice to everyone .\n",
      "100.0%\n",
      "good do , tinder ! i have undergone account verification two day ago and today i get _BAN_ for no reason . what a shame ! i not a tinder swindler ! i will keep my rating _NUMBER_ until my account recover . shame on you !\n",
      "100.0%\n",
      "literally get _BAN_ for no reason\n",
      "100.0%\n",
      "i get _BAN_ for no reason what so ever_EMOJI_\n",
      "100.0%\n",
      "i get _BAN_ for no reason .\n",
      "100.0%\n",
      "i get _BAN_ for no reason .\n",
      "==================================================\n",
      "Topic 1\n",
      "100.0%\n",
      "not waste your time or money on this app . customer support guy name larry be a milked teaspoon . app and support full of bellends\n",
      "100.0%\n",
      "every female on this app be either an escort ( i 've have _NUMBER_ escort _MATCHING_ with me in one day ) or a scamer ( i be stupid and not realize until it be too late and have _CUR_ _NUMBER_ dollar steal from me ) .. so in short not waste your time or money . just die alone its easier .\n",
      "100.0%\n",
      "this app be consciously upsetting pof piece of trash . i have be on this site many time . i ask to be show woman and it show me transgenders and ugly men in makeup . i do not now nor will ever date men as i be a man and bumping pickle be not part of life for me . now i understand why it be so horrible , the same people who own p.o.f . own this trash app also . do not waste your time nor money it be pointless as you can not pick and choose whom you want to communicate with .\n",
      "100.0%\n",
      "where do my _MESSAGE_ go ? that 's the last straw . this app do not just improve , it even consistently deteriorate . not waste time and money here . uninstalling ...\n",
      "100.0%\n",
      "be a pay customer , break no rule . do nothing wrong . get perma-_BAN_ at complete random . funny part ? they still keep my money , and this happen right after i pay . they swear up and down they refund me , i check with my bank several time , this be a complete lie . this be month ago , still nothing . they 're thief . not waste your time or money . company _LIKE_ this always get theirs in the end . read the other review . they be theives .\n",
      "100.0%\n",
      "bad app and very unsafe.i be try to buy _CHARGES_ but it slash almost _NUMBER_ k from my card without even otp.and even after that zero _MATCHING_.pls not waste your time or money over tinder .\n",
      "100.0%\n",
      "someone report my profile ( probably an ex ) tinder not review the _BAN_ , they just say that their user safety be their priority . not waste your time or money with this app .\n",
      "100.0%\n",
      "_LIKE_ all date apps these day , it 's all about pay money to be notice . as a decent look guy that get tell regularly that i not have a hard time meeting people , i not _MATCHING_ with hardly anyone ! ! ! not waste your time or money ... go out and meet people they way nature meant for us to\n",
      "100.0%\n",
      "most racist app ever ! ! ! ( google it ) _BUG_ only _MATCHING_ for caucasian not any other ethnicity even pay no _MATCHING_ or _LIKE_ . only show me caucasian people a barely africans & hispanic , no asian or other ethnic group and i 'm indiginous ! ! ! still have not relieve any type of customer service or feedback not waste your time , money , storage on this app if your anything besides a white person\n",
      "100.0%\n",
      "do not waste your time and money on this app .\n",
      "==================================================\n",
      "Topic 2\n",
      "100.0%\n",
      "so many _FAKE_ profile , no verification so it 's _SPAM_-ahoy . no thanks . delete .\n",
      "100.0%\n",
      "waste of time . so many _FAKE_ profile\n",
      "100.0%\n",
      "soooo many _FAKE_ profile on this app .\n",
      "100.0%\n",
      "use to be good , wish they id verify as there be so many _FAKE_ profile . competitor not have this issue . lot of men in the female card sort , business owner advertise etc . report not matter .\n",
      "100.0%\n",
      "too many _FAKE_ profile . a lot of _SCAM_ . need to tighten verification to avoid _FAKE_ user\n",
      "100.0%\n",
      "in korea , many _FAKE_ profile - > phishing _SCAM_\n",
      "100.0%\n",
      "dear tinder , instead of charge for every single thing why not you do something for authentication of the user . so many _FAKE_ profile on your app .\n",
      "100.0%\n",
      "fraud app . many _FAKE_ profile . disgust\n",
      "100.0%\n",
      "too many _FAKE_ profile . too _CHARGES_ .\n",
      "100.0%\n",
      "so many _FAKE_ profile ...\n",
      "==================================================\n",
      "Topic 3\n",
      "100.0%\n",
      "this app be good if you be attractive . if you not get anyone _MATCHING_ , it 's not because of the app it 's because you literally be not get any _MATCHING_ . woman on this app use it as if it 's a game , you talk to them for a while and never again .\n",
      "100.0%\n",
      "as your account get old .. u not get _MATCHING_ ... and when i delete and create again feature be pay ... which be previously not ... very irritate\n",
      "100.0%\n",
      "i not get _MATCHING_ _EMOJI_\n",
      "100.0%\n",
      "i 'm not get any _MATCHING_ .\n",
      "100.0%\n",
      "i 'm not get any _MATCHING_ . be i shadowbanned ? edit : i try that . still didnt work . can someone email me please ?\n",
      "100.0%\n",
      "beautiful woman but not get a _MATCHING_ .\n",
      "100.0%\n",
      "bad _BUG_ , after a week you not get any _MATCHING_ or new _LIKE_ . you 'll have to delete and create your account over and over again . thanks for get something that be good so disfunctional in such a short time\n",
      "100.0%\n",
      "fraud app i not get any _MATCHING_\n",
      "100.0%\n",
      "after _NUMBER_ year on this app i have only achieve _NUMBER_ _MATCHING_ . this app have encourage me to go to the gym every other day for the past _NUMBER_ month now . i may not be get any _MATCHING_ but atleast its motivated me to work out ( down to 168lb , im 6ft tall )\n",
      "100.0%\n",
      "i just not get _MATCHING_ man . i 'm just to damn ugly\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W, review_bad.tolist(), num_docs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8feba177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_2(line):\n",
    "    '''additional regex cleaning based on previous topic modeling result'''\n",
    "    line = re.sub(r'\\b(\\_emoji\\_)\\b', '', line)\n",
    "    line = re.sub(r'\\b\\_FAKE\\_ (?:account|profile)\\b', '_FAKE_', line)\n",
    "    line = re.sub(r'\\bwaste(?: time| money)?\\b', '_WASTE_', line)\n",
    "    line = re.sub(r'\\b(phone number)\\b', '_PHONE_', line)\n",
    "    line = re.sub(r'\\bget \\_MATCHING\\_\\b', '_MATCHING_', line)\n",
    "    line = re.sub(r'\\b(customer service)\\b','_CUSTOMER_SERVICE_', line)\n",
    "    line = re.sub(r'\\b(get _BAN_)\\b','_BAN_', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "40dde9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it be positively good and i meet with lot of nice , interesting , fun , loveable people , but there be a few pervert who on profile be not what they claim to be .',\n",
       " 'major user policy need change as a men try the apps , let me tell you it have full of broken soul , become an instagram follower mining , lot of wahmen will try to _SCAM_ you ... and as good not to mention the professional _SCAM_ that exist in most dating apps . if tinder still want to be relevant in these climate , major policy / user interaction need change . it have become a business platform for majority of the oposite sex . when apps not longer serve it purpose , it will die .',\n",
       " 'well',\n",
       " 'not so good',\n",
       " 'ton of _FAKE_ and everythings behind a _CHARGES_ . why would anyone be foolish enough to pay for a date app with so many _FAKE_ .']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_all = df['regex_1'].tolist()\n",
    "regex_2 = [regex_2(r) for r in review_all]\n",
    "regex_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "67a22f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['regex_2'] = regex_2\n",
    "review_good_2 = df[df['score'] >= 4]['regex_2']\n",
    "review_bad_2 = df[df['score'] < 4]['regex_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e986c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherryliu/opt/anaconda3/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC 0\n",
      "\n",
      "\bnot _waste_ time (58.8%)\n",
      "\n",
      "\b_ban_ without reason (1.2%)\n",
      "\n",
      "\b_fake_ not _waste_ (1.1%)\n",
      "\n",
      "\bplease not _waste_ (1.1%)\n",
      "\n",
      "\b_waste_ time not (0.8%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 1\n",
      "\n",
      "\baccount _ban_ reason (52.8%)\n",
      "\n",
      "\b_ban_ reason not (6.4%)\n",
      "\n",
      "\b_ban_ reason give (1.8%)\n",
      "\n",
      "\bsay account _ban_ (1.4%)\n",
      "\n",
      "\bgot _ban_ reason (1.2%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 2\n",
      "\n",
      "\b_number_ mile away (37.5%)\n",
      "\n",
      "\bshow _number_ mile (4.3%)\n",
      "\n",
      "\b_number_ _number_ mile (3.6%)\n",
      "\n",
      "\b_matching_ _number_ mile (3.5%)\n",
      "\n",
      "\bset _number_ mile (2.8%)\n",
      "\n",
      "==================================================\n",
      "TOPIC 3\n",
      "\n",
      "\b_waste_ time money (63.0%)\n",
      "\n",
      "\bdont _waste_ time (3.6%)\n",
      "\n",
      "\bcomplete _waste_ time (1.8%)\n",
      "\n",
      "\btime money not (1.6%)\n",
      "\n",
      "\btotal _waste_ time (1.5%)\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>not _waste_ time</th>\n",
       "      <td>309.422346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account _ban_ reason</th>\n",
       "      <td>294.387529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_waste_ time money</th>\n",
       "      <td>279.295139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_number_ mile away</th>\n",
       "      <td>278.928677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not get _matching_</th>\n",
       "      <td>211.434684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got _ban_ reason</th>\n",
       "      <td>206.943633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay see _like_</th>\n",
       "      <td>205.953549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ account reason</th>\n",
       "      <td>195.905838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ reason not</th>\n",
       "      <td>178.980233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>give _number_ star</th>\n",
       "      <td>173.808977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_ban_ without reason</th>\n",
       "      <td>166.003586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account _ban_ not</th>\n",
       "      <td>150.573584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not even get</th>\n",
       "      <td>145.298646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not _waste_ money</th>\n",
       "      <td>124.585022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not see _like_</th>\n",
       "      <td>119.063567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thousand mile away</th>\n",
       "      <td>118.823743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_number_ year old</th>\n",
       "      <td>116.827919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>say account _ban_</th>\n",
       "      <td>115.086071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay _cur_ _number_</th>\n",
       "      <td>114.173207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want money back</th>\n",
       "      <td>111.269711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           score\n",
       "not _waste_ time      309.422346\n",
       "account _ban_ reason  294.387529\n",
       "_waste_ time money    279.295139\n",
       "_number_ mile away    278.928677\n",
       "not get _matching_    211.434684\n",
       "got _ban_ reason      206.943633\n",
       "pay see _like_        205.953549\n",
       "_ban_ account reason  195.905838\n",
       "_ban_ reason not      178.980233\n",
       "give _number_ star    173.808977\n",
       "_ban_ without reason  166.003586\n",
       "account _ban_ not     150.573584\n",
       "not even get          145.298646\n",
       "not _waste_ money     124.585022\n",
       "not see _like_        119.063567\n",
       "thousand mile away    118.823743\n",
       "_number_ year old     116.827919\n",
       "say account _ban_     115.086071\n",
       "pay _cur_ _number_    114.173207\n",
       "want money back       111.269711"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, H, tf_idf_bad, score_bad_2 = get_tfidf_vec(review_bad_2, 3)\n",
    "score_bad_2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0332014c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "100.0%\n",
      "use hinge if your look to meet people . not waste your time with this poorly make app . the owner be a hellspawn ....\n",
      "100.0%\n",
      "tinder be full of _FAKE_ profile , _SPAM_ and totally rig to take your money , for practically nothing . absolute rubbish not waste your time guy 's ! !\n",
      "100.0%\n",
      "use to be good . not anymore , not waste your time instal it .\n",
      "100.0%\n",
      "the only people leave on this app be _FAKE_ and only fan catfishing account , not waste your time ...\n",
      "100.0%\n",
      "this be an ego-boosting app for girl . nothing more . not waste your time\n",
      "==================================================\n",
      "Topic 1\n",
      "100.0%\n",
      "account be _BAN_ for no reason\n",
      "100.0%\n",
      "my account get _BAN_ with out any reason .\n",
      "100.0%\n",
      "my account get _BAN_ for no reason only have it a day\n",
      "100.0%\n",
      "my account get _BAN_ for no reason . i love tinder .\n",
      "100.0%\n",
      "account get _BAN_ for no reason\n",
      "==================================================\n",
      "Topic 2\n",
      "100.0%\n",
      "every time they change their _CHARGES_ tier it just take stuff away from the bottom tier to force you to get the next one . literally take _NUMBER_ thing away from the bottom and only make them available on the other _NUMBER_. greedy mf ! also , it say this person be _NUMBER_ mile away and yet it 's more _LIKE_ _NUMBER_. damn thing make you turn on your gps just so it can be wrong by only _NUMBER_ state or so .\n",
      "100.0%\n",
      "tinder _PASSPORT_ be start to annoy me . too many user _NUMBER_ mile away . i want to see more profile of local people .\n",
      "100.0%\n",
      "require _LOCATION_ , but place me _NUMBER_ mile away from where i be . be this design to not work or whats the excuse for not let people set their city ?\n",
      "100.0%\n",
      "sort your _LOCATION_ out . why do i want to see woman _NUMBER_+ mile away .\n",
      "100.0%\n",
      "this app be horrible . _TOP_PICK_ be all _NUMBER_+ mile away which make them pointless and customer support be awful/almost nonexistent . the only reason i use this app be because it 's the only date app with many people on it . pof and bumble might work if you live in a population dense area but this be all i 've get .\n",
      "==================================================\n",
      "Topic 3\n",
      "100.0%\n",
      "stay off date apps . dating be a waste of time and money . walk away gentleman .\n",
      "100.0%\n",
      "_NUMBER_ % of `` user '' be _FAKE_ , it 's super glitchy , constantly give _FAKE_ _SPAM_ so that they can bombard you with their _CHARGES_ offer , and to top it all off your account will get _BAN_ randomly . i be in a relationship for about _NUMBER_ month , redownloaded the app and my account be immediately _BAN_ . i 've read all the guideline , and ig my account get _BAN_ for inactivity ? not worth waste your time or money on this app .\n",
      "100.0%\n",
      "waste of time and money\n",
      "100.0%\n",
      "just as bad a pay to win mobile game , log you out for no reason , will shadowban you without warn and/or wont show your profile to anybody and there practically no customer service , they dont even have a phone number you can call , this app will literally waste your time and money , your good off just out and just meeting somebody\n",
      "100.0%\n",
      "even after month of _SWIPE_ . gold _CHARGES_ , you never seem to find real people here . mostly _FAKE_ or old profile . noone reply and it 's a waste of time and money .\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "get_top_documents_for_each_topic(W, review_bad.tolist(), num_docs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "d8acedbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67806, 1)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bad = review_bad_2.to_frame()\n",
    "df_bad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "66eba6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad['ban'] = df_bad['regex_2'].apply(lambda x: 1 if x.find('_BAN_') != -1 else 0)\n",
    "df_ban = df_bad[df_bad['ban'] == 1]\n",
    "ban = df_ban.regex_2.tolist()\n",
    "ban = [re.sub(r'\\b\\_BAN\\_\\b', '', line) for line in ban]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "92d6f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vec_sec(review, i):\n",
    "    '''Intake a list of review and return tf-idf report'''\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(i,i),\n",
    "                             token_pattern=r'\\b[a-zA-Z\\_]{3,}\\b',\n",
    "                             max_df=0.4, stop_words=stopwords, max_features=1000, binary=True)\n",
    "    X = vectorizer.fit_transform(review)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "    tf_idf_sum = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf_sum, columns=[\"score\"])\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    return tf_idf.transpose(), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a1bc6ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>account reason</th>\n",
       "      <td>322.233003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not even</th>\n",
       "      <td>261.398554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason not</th>\n",
       "      <td>192.760472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>without reason</th>\n",
       "      <td>163.665721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>got reason</th>\n",
       "      <td>163.509281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account not</th>\n",
       "      <td>158.551520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not know</th>\n",
       "      <td>144.443771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>appeal process</th>\n",
       "      <td>127.944682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not use</th>\n",
       "      <td>124.954271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_number_ year</th>\n",
       "      <td>122.451651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account without</th>\n",
       "      <td>121.871018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>violate term</th>\n",
       "      <td>115.942707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not anything</th>\n",
       "      <td>111.140335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pay _charges_</th>\n",
       "      <td>104.451150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not tell</th>\n",
       "      <td>101.173188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_number_ star</th>\n",
       "      <td>96.109748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tinder not</th>\n",
       "      <td>93.525077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_waste_ time</th>\n",
       "      <td>92.902509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make account</th>\n",
       "      <td>92.206573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>use tinder</th>\n",
       "      <td>91.825774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      score\n",
       "account reason   322.233003\n",
       "not even         261.398554\n",
       "reason not       192.760472\n",
       "without reason   163.665721\n",
       "got reason       163.509281\n",
       "account not      158.551520\n",
       "not know         144.443771\n",
       "appeal process   127.944682\n",
       "not use          124.954271\n",
       "_number_ year    122.451651\n",
       "account without  121.871018\n",
       "violate term     115.942707\n",
       "not anything     111.140335\n",
       "pay _charges_    104.451150\n",
       "not tell         101.173188\n",
       "_number_ star     96.109748\n",
       "tinder not        93.525077\n",
       "_waste_ time      92.902509\n",
       "make account      92.206573\n",
       "use tinder        91.825774"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_ban, score_ban = get_tfidf_vec_sec(ban, 2)\n",
    "score_ban[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd02205790e15d2e0e7826f552575388932e4ce5a7ab9e16340e28300e3e5bd9db7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
